{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \n",
    "    with open(path) as file_data:\n",
    "        data = list()\n",
    "\n",
    "        for line in file_data:\n",
    "            data.append([float(d) for d in line.split()])\n",
    "\n",
    "        data = np.array(data)\n",
    "        X = data[:,1:-1]\n",
    "        Y = data[:,-1]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_add_ones(X):\n",
    "    X = np.array(X)\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    \n",
    "    X_normalized = (X - X_min) / (X_max - X_min)\n",
    "    ones = np.array([[1] for i in range (X_normalized.shape[0])])\n",
    "\n",
    "    return np.column_stack((ones, X_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def fit(self, X_train, Y_train, LAMBDA):\n",
    "        assert len(X_train.shape) == 2 and X_train.shape[0] == Y_train.shape[0]\n",
    "\n",
    "        W = np.linalg.inv(X_train.transpose().dot(X_train) + LAMBDA * np.identity(X_train.shape[1])).dot(X_train.transpose()).dot(Y_train)\n",
    "        \n",
    "        return W\n",
    "\n",
    "    def fit_gradient_descent(self, X_train, Y_train, LAMBDA, learning_rate, max_num_epoch = 1000, batch_size = 128):\n",
    "        W = np.random.randn(X_train.shape[1])\n",
    "        last_loss = 10e8\n",
    "\n",
    "        for ep in range(max_num_epoch):\n",
    "\n",
    "            arr = np.array(range(X_train.shape[0]))\n",
    "            np.random.shuffle(arr)\n",
    "            X_train = X_train[arr]\n",
    "            Y_train = Y_train[arr]\n",
    "            total_minibatch = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "            for i in range(total_minibatch):\n",
    "                index = i * batch_size\n",
    "                X_train_sub = X_train[index:index+batch_size]\n",
    "                Y_train_sub = Y_train[index:index+batch_size]\n",
    "                grad = X_train_sub.T.dot(X_train_sub.dot(W) - Y_train_sub) + LAMBDA * W\n",
    "                W = W - learning_rate*grad\n",
    "            \n",
    "            new_loss = self.compute_RSS(self.predict(W, X_train), Y_train)\n",
    "\n",
    "            if np.abs(new_loss - last_loss) <= 1e-5:\n",
    "                break\n",
    "            last_loss = new_loss\n",
    "        \n",
    "        return W\n",
    "\n",
    "\n",
    "    def predict(self, W, X_new):\n",
    "        X_new = np.array(X_new)\n",
    "        Y_new = X_new.dot(W)\n",
    "\n",
    "        return Y_new\n",
    "\n",
    "    def compute_RSS(self, Y_new, Y_predicted):\n",
    "        loss = 1. / Y_new.shape[0] * np.sum((Y_new - Y_predicted) ** 2)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_the_best_LAMBDA(self, X_train, Y_train):\n",
    "        def cross_validation(num_folds, LAMBDA):\n",
    "            row_ids = np.array(range(X_train.shape[0]))\n",
    "\n",
    "            valid_ids = np.split(row_ids[:len(row_ids) - len(row_ids) % num_folds], num_folds)\n",
    "            valid_ids[-1] = np.append(valid_ids[-1], row_ids[len(row_ids) - len(row_ids) % num_folds:])\n",
    "            train_ids = [[k for k in row_ids if k not in valid_ids[i]] for i in range(num_folds)]\n",
    "            aver_RSS = 0\n",
    "\n",
    "            for i in range(num_folds):\n",
    "                valid_part = {'X': X_train[valid_ids[i]], 'Y': Y_train[valid_ids[i]]}\n",
    "                train_part = {'X': X_train[train_ids[i]], 'Y': Y_train[train_ids[i]]}\n",
    "                W = self.fit(train_part['X'], train_part['Y'], LAMBDA)\n",
    "                Y_predicted = self.predict(W, valid_part['X'])\n",
    "                aver_RSS += self.compute_RSS(valid_part['Y'], Y_predicted)\n",
    "\n",
    "            return aver_RSS / num_folds\n",
    "\n",
    "        def range_scan(best_LAMBDA, minimum_RSS, LAMBDA_values):\n",
    "            for current_LAMBDA in LAMBDA_values:\n",
    "                aver_RSS = cross_validation(num_folds=5, LAMBDA=current_LAMBDA)\n",
    "                if aver_RSS < minimum_RSS:\n",
    "                    best_LAMBDA = current_LAMBDA\n",
    "                    minimum_RSS = aver_RSS\n",
    "\n",
    "            return best_LAMBDA, minimum_RSS\n",
    "\n",
    "\n",
    "        # Choose initial lambda from range MIN_LAMBDA -> MAX_LAMBDA\n",
    "        MIN_LAMBDA = 0\n",
    "        MAX_LAMBDA = 50\n",
    "        best_LAMBDA, minimum_RSS = range_scan(best_LAMBDA=0, minimum_RSS=10000**2, LAMBDA_values=range(MIN_LAMBDA, MAX_LAMBDA, 1))\n",
    "\n",
    "        # print(\"Initial lambda:\", best_LAMBDA)\n",
    "\n",
    "        # Choose lambda in a smaller range\n",
    "        LAMBDA_values = [k * 1. /1000 for k in range (\n",
    "            max(0, (best_LAMBDA - 1) * 1000), (best_LAMBDA + 1) * 1000, 1\n",
    "        )]\n",
    "\n",
    "        best_LAMBDA, minimum_RSS = range_scan(best_LAMBDA=best_LAMBDA, minimum_RSS=minimum_RSS, LAMBDA_values=LAMBDA_values)\n",
    "\n",
    "        return best_LAMBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LAMBDA: 0.002\n",
      "1527.0698078049916\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    X, Y = read_data(path='../datasets/death-rates.txt')\n",
    "    \n",
    "    # Normalization\n",
    "    X = normalize_and_add_ones(X)\n",
    "    X_train, Y_train = X[:50], Y[:50]\n",
    "    X_test, Y_test = X[50:], Y[50:]\n",
    "\n",
    "    ridge_regression = RidgeRegression()\n",
    "    best_LAMBDA = ridge_regression.get_the_best_LAMBDA(X_train, Y_train)\n",
    "\n",
    "    print('Best LAMBDA:', best_LAMBDA)\n",
    "\n",
    "    W_learned = ridge_regression.fit(\n",
    "        X_train=X_train, Y_train=Y_train, LAMBDA=best_LAMBDA\n",
    "    )\n",
    "   \n",
    "    # W_learned = ridge_regression.fit_gradient_descent(X_train=X_train, Y_train=Y_train, LAMBDA=best_LAMBDA, learning_rate=0.2)\n",
    "\n",
    "    Y_predicted = ridge_regression.predict(W=W_learned, X_new=X_test)\n",
    "\n",
    "    print(ridge_regression.compute_RSS(Y_new=Y_test, Y_predicted=Y_predicted))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "925c48296191328fd63282b32f31d58366fe2156a0e6251ae4d08cd1c8bc0f76"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
